# plugins used in this pipeline:
#   + logstash-input-syslog
#   + logstash-filter-kv
#   + logstash-filter-date
#   + logstash-filter-mutate
#   + logstash-filter-geoip
#   + logstash-filter-ruby
#   + logstash-output-elasticsearch
#
# index template settings:
#   + Data stream: "true"
#   + Priority: "200"
#   + Component templates: "as needed ..."
#   + Index settings: "specify ILM Policy"

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~
# >> Syslog RFC 5424 Input <<
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~
input {
  syslog {
    port => 9002
    grok_pattern => "<%{POSINT:pri}>%{NUMBER:version} %{TIMESTAMP_ISO8601:timestamp} %{SYSLOGHOST:logsource} %{DATA:application} %{DATA:pid} %{DATA:message_id} \[%{GREEDYDATA:sdata}\]%{GREEDYDATA:msg}"
  }
}

# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
# >> Parse F5 AFM Log Format <<
# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
filter {
  # Extract Key Value Pairs from structured Data
  kv {
    source => "sdata"
    value_split => "="
    trim_key => " "
  }

  # Parse correct Timestamp
  date {
    match => [ "timestamp", "ISO8601" ]
  }

  mutate {
    # Add event.dataset
    add_field => { "[event][dataset]" => "f5.afm"  }

    # Rename Fields
    rename => { "dest_ip" => "dst" }
    rename => { "dest_port" => "d_port" }
    rename => { "source_ip" => "src" }
    rename => { "src_port" => "s_port" }

    # Remove unnecessary Fields
    remove_field => [ "sdata", "msg" ]
    remove_field => [ "dst_geo", "src_geo" ]
  }

  # Add geo information for destination IP
  if [dst] {
    geoip {
      source => "dst"
      target => "dst_geo"
      tag_on_failure => [ ]
    }
  }

  # Add geo information for source IP
  if [src] {
    geoip {
      source => "src"
      target => "src_geo"
      tag_on_failure => [ ]
    }
  }

  # Remove empty Fields
  ruby {
    code => "
      hash = event.to_hash
      hash.each do |k,v|
        if v == '' || v == nil || v == 'unknown'
          event.remove(k)
        end
      end
    "
  }
}

# ~~~~~~~~~~~~~~~~~~~~
# >> Elastic Output <<
# ~~~~~~~~~~~~~~~~~~~~
output {
  if ![tags] {
    # on success => write to elastic
    elasticsearch {
      hosts => ["https://127.0.0.1:9200"]
      index => "logs-%{[event][dataset]}-prod"
      action => "create"
    }
  }
  else {
    # on failure => write to logfile
    file {
      path => "/var/log/logstash/pipeline.log"
    }
  }
}
